{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.figsize'] = 6, 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 (5+5+5+5+5=25 pts)\n",
    "\n",
    "### Consider the dataset HW2_task1.csv, a binary classification problem with two real-valued input attributes.  As you can see from the plot, the dataset is linearly separable. Train a linear SVM (setting C=100000 just to emphasize that no slack variables are allowed) and  answer the following five questions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "data1=pd.read_csv('HW2_task1.csv')\n",
    "X=data1.iloc[:,:2]\n",
    "Y=data1.iloc[:,2]\n",
    "plt.gca()\n",
    "plt.scatter(X.iloc[:,0], X.iloc[:,1], s=50, c=Y, cmap=plt.cm.get_cmap('coolwarm', 2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Report the separating hyperplane (line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. List the support vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Calculate the upper and lower hyperplanes (lines) of the margin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Compute the width of the margin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e. What would you expect to happen to the margin if the constant C was made very small?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here.  You can justify your intuitions with code if you'd like, but this is not required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 (4+8+8=20 pts)\n",
    "\n",
    "### Given the dataset provided below, answer the following two questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data \n",
    "data2=pd.read_csv('HW2_task2.csv')\n",
    "X=data2.iloc[:,:2]\n",
    "Y=data2.iloc[:,2]\n",
    "\n",
    "plt.gca()\n",
    "plt.scatter(X.iloc[:,0], X.iloc[:,1], s=50, c=Y, cmap=plt.cm.get_cmap('coolwarm', 2));\n",
    "plt.show()\n",
    "\n",
    "# Generate training (X_train, Y_train) and testing (X_test, Y_test) datasets for out of sample test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Train a polynomial SVM using the training set. Use the default arguments, and report both in-sample (training set) and out-of-sample (test set) classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next we will use the validation subset in order to pick the optimal parameters for the polynomial model.\n",
    "\n",
    "### b. Try polynomials of degree 1,2,3,4. For each degree, consider a variety of regularization constants from the range C=[math.exp(i) for i in np.linspace(-10,2*degree,200)] in order to evaluate the classifier performance over the validation set defined below.  Plot the graph of \"Accuracy vs log(C)\" for each degree.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Next we generate validation data (X_vali, Y_vali) from the training dataset. \n",
    "# Denote the remaining training data by (X_train_1, Y_train_1).\n",
    "X_train_1,X_vali,Y_train_1,Y_vali = train_test_split(X_train, Y_train, test_size=0.33, random_state=99)\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Choose the optimal degree and the optimal regularization constant C based on these graphs.  Use the optimal degree and C to compute and report the final out-of-sample accuracy of the best classification model selected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 (5+5=10pts).\n",
    "\n",
    "### This task is to be done by hand rather than using Python.\n",
    "\n",
    "Assume you have a data set as below. It contains records of cars with three features: the type of the car (sports or SUV), the color of the car (red or yellow), and the origin of the car (domestic or imported). And the labels for the data are yes (car was stolen) and no (car was not stolen).\n",
    "\n",
    "CarType,Color,Origin,Stolen?\n",
    "\n",
    "sports,red,domestic,yes\n",
    "\n",
    "sports,red,domestic,yes\n",
    "\n",
    "sports,red,domestic,yes\n",
    "\n",
    "sports,red,domestic,no\n",
    "\n",
    "SUV,red,domestic,no\n",
    "\n",
    "SUV,red,imported,yes\n",
    "\n",
    "SUV,yellow,imported,no\n",
    "\n",
    "SUV,yellow,imported,yes\n",
    "\n",
    "SUV,yellow,domestic,no\n",
    "\n",
    "sports,yellow,imported,no\n",
    "\n",
    "sports,red,imported,yes\n",
    "\n",
    "\n",
    "### a) Calculate the following sample probabilities:\n",
    "\n",
    "P(Yes)\n",
    "\n",
    "P(No)\n",
    "\n",
    "P(Red|Yes)\n",
    "\n",
    "P(SUV|Yes)\n",
    "\n",
    "P(Domestic|Yes)\n",
    "\n",
    "P(Red|No)\n",
    "\n",
    "P(SUV|No)\n",
    "\n",
    "P(Domestic|No)\n",
    "\n",
    "### b) Using naive Bayes classification, what is the probability that a red, domestic SUV will be stolen? Show your work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 (4+2+2+2=10pts)\n",
    "\n",
    "Consider the following problem involving Gaussian Naive Bayes classification.  We use eight factors to predict if people have diabetes or not. The variables are:\n",
    "\n",
    "y: The label (0 - no diabetes, 1 - diabetes)\n",
    "\n",
    "t_pre: Number of times pregnant\n",
    "\n",
    "glu: Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "\n",
    "blood_p: Diastolic blood pressure (mm Hg)\n",
    "\n",
    "triceps: Triceps skin fold thickness (mm)\n",
    "\n",
    "serum: 2-Hour serum insulin (mu U/ml)\n",
    "\n",
    "b_m: Body mass index (weight in kg/(height in m)^2)\n",
    "\n",
    "pedigree_f: Diabetes pedigree function\n",
    "\n",
    "age: Age (years)\n",
    "\n",
    "### Using the data provided below, learn a Naive Bayes classifier from the training data and answer the following questions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "data_train=pd.read_csv(\"HW2_task4_train.csv\") \n",
    "y_train=data_train.iloc[:,1] \n",
    "X_train=data_train.iloc[:,2:] \n",
    "\n",
    "# Testing data\n",
    "data_test=pd.read_csv(\"HW2_task4_test.csv\")\n",
    "y_test=data_test.iloc[:,1]\n",
    "X_test=data_test.iloc[:,2:]\n",
    "\n",
    "print X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) What is the prediction accuracy for Naive Bayes, both in sample (on the training data) and out of sample (on the test data)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) What is the prior probability of diabetes, learned from the training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) What is the mean and variance of each input variable for patients with diabetes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) What is the mean and variance of each input variable for patients without diabetes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5 (10+15=25 pts)\n",
    "\n",
    "We have an artificial data set split, where the training set contains both labeled and unlabeled data. Column 'y' is the label, and columns '0','1','2' are categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train=pd.read_csv(\"HW2_task5_train.csv\")\n",
    "y_Labeled_train=data_train.iloc[:,1] \n",
    "X_Labeled_train=data_train.iloc[:,2:] \n",
    "\n",
    "data_test=pd.read_csv(\"HW2_task5_test.csv\")\n",
    "y_Labeled_test=data_test.iloc[:,1]\n",
    "X_Labeled_test=data_test.iloc[:,2:]\n",
    "\n",
    "data_Unlabeled=pd.read_csv(\"HW2_task5_unlabeled.csv\")\n",
    "X_Unlabeled_train=data_Unlabeled.iloc[:,1:]\n",
    "\n",
    "print X_Labeled_train\n",
    "print y_Labeled_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Learn a discrete Naive Bayes classifier from  X_Labeled_train, use it to predict the labels of X_Labeled_test, and report the classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Improve the classification by using the unlabeled data, data_Unlabeled, and the EM semi-supervised algorithm to predict the labels of X_Labeled_test, and report the new accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
